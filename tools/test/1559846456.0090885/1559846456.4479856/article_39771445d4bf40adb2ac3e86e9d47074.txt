Incrementally”。
它主要解决了一个深度学习中的重要问题：如何使用尽可能少的标签数据来训练一个效果promising的分类器。 
B一般发生在高富帅公司，有海量的精标记数据，但是由于目前即使是最牛逼的计算机也不能用深度学习在短时间内一次性地去处理完这些数据（e.g.，内存溢出，或者算上个几年都算不完）。 
Anyway，我想我已经说清楚应用背景了，读者可以根据实际情况判断是否往后读下去。 
感谢你选择继续往下阅读，那么如果觉得下文所传递的思想和方法对你有帮助，请记得一定引用这篇CVPR2017的文章。 
**1\. 为什么会想到去解决这个问题？ 
现在深度学习很火，做的人也越来越多，那么它的门槛可以说是很低的，Caffe，Keras，Torch等等框架的出现，让该领域的programming的门槛直接拆了。
所以深度学习真正的门槛变成了很简单概念——钱。 
Power），二是标记数据的数量。
这儿就引出一个很重要的问题：是不是训练数据集越多，深度学习的效果会越好呢？
这个答案凭空想是想不出来的，能回答的人一定是已经拥有了海量的数据，如ImageNet，Place等等，他们可以做一系列的实验来回答这个问题。 
truth就是在浪费时间和金钱。
有了这个认知，接下来就是想办法让这个临界值变小，也就是用更小的训练集来更快地达到最理想的性能，如右图的红虚线所示。 
Learning）的手段来增加训练集，从而找到一个更小的子集来达到最理想的性能。 
这里需要说明的一点是，训练样本数的临界点大小和这个分类问题的难度有关，如果这个分类问题非常简单，如黑白图像分类（白色的是1，黑色的是0），那么这个临界值就特别小，往往几幅图就可以训练一个精度很高的分类器；如果分类问题很复杂，如判断一个肿瘤的良恶性（良性是0，恶性是1），那么这个临界值会很大，因为肿瘤的形状，大小，位置各异，分类器需要学习很多很多的样本，才能达到一个比较稳定的性能。 
FIG.1对于很多从事深度学习的无论是研究员还是企业家都是一个十分有启发性的认知改变。
一般来讲，人的惯性思维会引领一个默认的思路，就是训练样本越多越好，如左图所示，这将直接导致许多工作的停滞不前，理由永远是“我们没有足够的数据，怎么训练网络！
”进一步的思路是图二的红实线认知：要多少是多啊，先训着再说，慢慢的就会发现即便用自己有的一小部分数据集好像也能达到一个不错的分类性能，这个时候就遇到一个问题：自己的数据集样本数到底有没有到达临界值呢？
这个问题也很关键，它决定了要不要继续花钱去找人标注数据了。
这个问题我会在第三部分去回答它，这里先假设我们知道了它的答案，接下来的问题就是如何让这个临界值变小？ 
**2\. 如何让临界值变小？ 
mining）。 
sample对于提升分类器效果最有效而快速。 
sample？
或者说怎么去描述当前分类器对于不同样本的分类结果的好坏？ 
定义：由于深度学习的输出是属于某一类的概率（0～1），一个很直观的方法就是用“熵（entropy）”来刻画信息量，把那些预测值模棱两可的样本挑出来，对于二分类问题，就是预测值越靠近0.5，它们的信息量越大。 
data和unlabeled data的相似性。 
with guaranteed solution bounds”中被提出。
是十分重要的两个Active Learning的选择指标。 
有了这两个指标来选hard sample，是比较靠谱了——实验结果表明，这比随机去选已经能更快地达到临界拐点了。 
FIG.2 Active Learning的结构示意图。
利用深度学习所带来的优势在于，一开始你可以不需要有标记的数据集。 
举例来讲，假设你是一个养狗的大户，你现在想做一个非常偏的（专业化的）分类问题，如卷毛比雄犬和哈瓦那犬的分类问题，你手头有这两种狗各50条，你自己可以很轻松地区分这100条狗，现在需要做的是训练一个分类器，给那些不懂狗的人，他们拍了狗的照片然后把照片输入到这个分类器就可以很好地判断这是卷毛比雄犬还是哈瓦那犬。
首先你可以给这100条狗拍照片，每条狗都有不同形态的10张照片，一共拍了1000张没有标记的照片。
对于这1000张照片，你所知道的是哪10张对应的是一条狗，其他什么都不知道。 
Image也不能企及，当然一种选择是你把1000张图片从头到尾看一遍，标注好，但是你更希望是把大多数简单的分类工作交给分类器，自己尽可能少的做标记工作，并且主要是去标记那些分类器模棱两可的那几张照片来提高分类器的性能。 
如FIG.2所示，每次循环都用不断增加的标记数据集去提升分类器的性能，每次都挑对当前分类器比较难的样本来人为标记。 
**3\. 这个过程什么时候可以停？ 
以上三种情况都可以让这个循环训练过程中断，第一种就很无奈了，没钱找人标记了...第二种情况和第三种情况的前提共识是如果难的样本都分类正确了，那么我们认为简单的样本肯定也基本上分类正确了，即便不知道标签。
第三种情况，举例来说就是黑白图像分类，结果分类器模棱两可的图像是灰的...也就是说事实上的确分不了，并且当前的分类器居然能把分不了的样本也找出来，这时我们认为这个分类器的性能已经不错的了，所以循环训练结束。 
至此，主要讲了传统的Active Learning的思想，接下来会讲讲这篇CVPR2017论文的几个工作点。 
Painters。 
Learning的技术完全可以直接用啦。
但是实际上并不是这样的，上面说的熵（Entropy）的确没有什么严重的问题，但是多样性（Diversity），在很多现实应用中问题就会出现。 
data比较相似的作为简单样本，每次active select难样本，也就是挑出来和labeled data不太像的出来。 
data，在它们组成的大矩阵中找出最优的子矩阵。
这个方法在理论上是可行的，但是实际应用中，数据量（labeled和unlabeled）会非常大，这个矩阵会特别的大，导致求最优解会很慢，或者根本得不出来最优解。 
level上算diversity。 
Augmentation是必须的环节，我们就抓住了这个点来设计Diversity这个指标。 
augmentation后的patches，从CNN出来的预测值应该相对是一致的，因为它们的truth应该还是一致的。
比如一张猫的图像，经过数据扩充，得到的那些个patch所对应的truth也应该都是猫。 
定义：对于来自同一幅image的patch集，如果它们的分类结果高度不统一了，那么这个image就是Important的，或者hard sample。 
  1. 由于在annotation之前不知道label，所以我们不能知道网络的预测正确还是错误，但是我们可以知道预测统一还是不统一。
所以比如一幅猫的图，如果网络的预测很统一都是狗，那么我们也认为这是一个easy sample，不去active select它的。 
  2. 结合data augmentation的优点是我们可以知道哪些patch对应什么image，比较容易控制。
这样就可以在一个image內算diversity了，每个image对应一个矩阵，大小是一样的，非常的简洁，也容易控制计算量。 
这样的diversity就完美了吗？
并没有... 读者可以先猜猜哪儿又出问题啦，我在第五部分会指出来。 
问题出在上面的假设：经过data augmentation后的patches，从CNN出来的预测值应该相对是一致的，因为它们的truth应该还是一致的。 
我们知道有几种经典的数据扩充方法：平移，旋转，缩放，形变，加噪声等等。
但是很有可能发生的是，经过这些变化以后，得到的patch是无法分类的。 
[图片上传失败...(image-3654fa-1512163792625)]，右图是网络得到的对应patch的预测值。 
sample挑出来。 
be rat, rabbit, etc. 我们把像这三个patch的例子叫做从data augmentation带来的noisy label issue。 
而对于4～6的patches，网络很好地做出了分类，这很合情合理。 
Augmentation）带来的干扰样本称为noisy labels。
Fig.4只是一个很直观的例子，其实在实际的数据集中会有很多这样的案例。 
我们给出的解决方法是：先计算majority的预测，然后在majority上面算diversity，只要网络的预测大方向是统一的，就是统一的。
意思就是想个办法把Fig.4中的三个非主流0.1扔掉。 
sample挑出来，因为当前的分类器实际上已经可以分出来这幅图的类别啦。 
0，如果是前者，那么就从大到小取前25%的预测，其他的不要啦，如果是后者，就从小到大取前25%，其他的不要啦。 
sample挑出来啦。
成功解决了data augmentation带来的noisy label issue。 
Selection部分结束。 
6\. 如何训练？ 
Learning的效果会没有random selecting好。 
Learning的效果会一下子超过random selecting。 
tuning)。 
tuning，但是缺点是随着labeled数据量大增加，GPU的消耗很大，相当于每次有新的标注数据来的时候，就把原来的model扔了不管，在实际应用中的代价还是很大的。
第二种方法是从当前的model基础上做finetune，在某种意义上knowledge是有记忆的，而且是连续渐进式的学习。 
rate，需要适当的减小，而且比较容易在一开始掉入local minimum。 
Finetuning的前期论文也是有的，需要更进一步的研究。 
learning，只是这样一个网络结构去不断的学习吗，还是随着数据集的增加去让网络自适应的变深，便复杂，都是我比较关心的问题。 
question有待研究，因此，只能说我们的论文是active learning的一个引子。 
本文只是从思想的角度来阐述这篇论文，至于后续的分析，结果和结论都在论文中详细的report了。



