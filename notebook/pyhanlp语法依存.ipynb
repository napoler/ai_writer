{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/terry/pan/github/ai_writer/lib/python3.6/site-packages\n",
      "Requirement already satisfied: six in /home/terry/pan/github/ai_writer/lib/python3.6/site-packages (from nltk)\n",
      "Requirement already satisfied: stanfordcorenlp in /home/terry/pan/github/ai_writer/lib/python3.6/site-packages\n",
      "Requirement already satisfied: requests in /home/terry/pan/github/ai_writer/lib/python3.6/site-packages (from stanfordcorenlp)\n",
      "Requirement already satisfied: psutil in /home/terry/pan/github/ai_writer/lib/python3.6/site-packages (from stanfordcorenlp)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/terry/pan/github/ai_writer/lib/python3.6/site-packages (from requests->stanfordcorenlp)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/terry/pan/github/ai_writer/lib/python3.6/site-packages (from requests->stanfordcorenlp)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/terry/pan/github/ai_writer/lib/python3.6/site-packages (from requests->stanfordcorenlp)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/terry/pan/github/ai_writer/lib/python3.6/site-packages (from requests->stanfordcorenlp)\n",
      "Collecting harvesttext\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1b/4f/966d1917b4640d9e185866a17c07152bd069eb49fa7de1659cef8526fb14/harvesttext-0.5.4.1-py3-none-any.whl (1.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.8MB 932kB/s \n",
      "\u001b[?25hRequirement already satisfied: jieba in /home/terry/pan/github/ai_writer/lib/python3.6/site-packages (from harvesttext)\n",
      "Requirement already satisfied: numpy in /home/terry/pan/github/ai_writer/lib/python3.6/site-packages (from harvesttext)\n",
      "Collecting pypinyin (from harvesttext)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/bb/7d/2ed2011b6951f59146148a0184d9a239640cb8487a9fa6d16b94bc71244f/pypinyin-0.35.3-py2.py3-none-any.whl (770kB)\n",
      "\u001b[K    100% |████████████████████████████████| 778kB 2.1MB/s \n",
      "\u001b[?25hCollecting pandas (from harvesttext)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/19/74/e50234bc82c553fecdbd566d8650801e3fe2d6d8c8d940638e3d8a7c5522/pandas-0.24.2-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 10.1MB 196kB/s \n",
      "\u001b[?25hCollecting pyxDamerauLevenshtein==1.5 (from harvesttext)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/09/d8/77d02800d687ff8e12c8ec7b4ed917249fca27a1bccc6d24f0ac507a794c/pyxDamerauLevenshtein-1.5.tar.gz (54kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 8.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyhanlp in /home/terry/pan/github/ai_writer/lib/python3.6/site-packages (from harvesttext)\n",
      "Collecting rdflib (from harvesttext)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3c/fe/630bacb652680f6d481b9febbb3e2c3869194a1a5fc3401a4a41195a2f8f/rdflib-4.2.2-py3-none-any.whl (344kB)\n",
      "\u001b[K    100% |████████████████████████████████| 348kB 4.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: networkx in /home/terry/pan/github/ai_writer/lib/python3.6/site-packages (from harvesttext)\n",
      "Collecting pytz>=2011k (from pandas->harvesttext)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3d/73/fe30c2daaaa0713420d0382b16fbb761409f532c56bdcc514bf7b6262bb6/pytz-2019.1-py2.py3-none-any.whl (510kB)\n",
      "\u001b[K    100% |████████████████████████████████| 512kB 3.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.0 in /home/terry/pan/github/ai_writer/lib/python3.6/site-packages (from pandas->harvesttext)\n",
      "Requirement already satisfied: jpype1 in /home/terry/pan/github/ai_writer/lib/python3.6/site-packages (from pyhanlp->harvesttext)\n",
      "Collecting isodate (from rdflib->harvesttext)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 10.9MB/s \n",
      "\u001b[?25hCollecting pyparsing (from rdflib->harvesttext)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/dd/d9/3ec19e966301a6e25769976999bd7bbe552016f0d32b577dc9d63d2e0c49/pyparsing-2.4.0-py2.py3-none-any.whl (62kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 10.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /home/terry/pan/github/ai_writer/lib/python3.6/site-packages (from networkx->harvesttext)\n",
      "Requirement already satisfied: six>=1.5 in /home/terry/pan/github/ai_writer/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas->harvesttext)\n",
      "Building wheels for collected packages: pyxDamerauLevenshtein\n",
      "  Running setup.py bdist_wheel for pyxDamerauLevenshtein ... \u001b[?25lerror\n",
      "  Complete output from command /home/terry/pan/github/ai_writer/bin/python3 -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-slzfr3wu/pyxDamerauLevenshtein/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /tmp/tmpg0b3yvtppip-wheel- --python-tag cp36:\n",
      "  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n",
      "     or: -c --help [cmd1 cmd2 ...]\n",
      "     or: -c --help-commands\n",
      "     or: -c cmd --help\n",
      "  \n",
      "  error: invalid command 'bdist_wheel'\n",
      "  \n",
      "  ----------------------------------------\n",
      "\u001b[31m  Failed building wheel for pyxDamerauLevenshtein\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for pyxDamerauLevenshtein\n",
      "Failed to build pyxDamerauLevenshtein\n",
      "Installing collected packages: pypinyin, pytz, pandas, pyxDamerauLevenshtein, isodate, pyparsing, rdflib, harvesttext\n",
      "  Running setup.py install for pyxDamerauLevenshtein ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed harvesttext-0.5.4.1 isodate-0.6.0 pandas-0.24.2 pyparsing-2.4.0 pypinyin-0.35.3 pytz-2019.1 pyxDamerauLevenshtein-1.5 rdflib-4.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk\n",
    "!pip3 install stanfordcorenlp\n",
    "!pip3 install harvesttext\n",
    "\n",
    "# https://github.com/blmoistawinde/HarvestText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/terry/pan/github/ai_writer/ai_writer'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 切换工作目录\n",
    "import os\n",
    "os.chdir(\"../\")   #修改当前工作目录\n",
    "os.getcwd()    #获取当前工作目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "java.util.NoSuchElementExceptionPyRaisable",
     "evalue": "java.util.NoSuchElementException",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mjava.util.NoSuchElementExceptionPyRaisable\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2787b5f918a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mdemo_dependency_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;31m#     import doctest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#     doctest.testmod(verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-2787b5f918a9>\u001b[0m in \u001b[0;36mdemo_dependency_parser\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mgithub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mhankcs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mpyhanlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"\"\"\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHanLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseDependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"天喂饲食物的量要适度，过多过少都不利成长。\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m#     print(dir())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 通过dir()可以查看sentence的方法\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mjava.util.NoSuchElementExceptionPyRaisable\u001b[0m: java.util.NoSuchElementException"
     ]
    }
   ],
   "source": [
    "# # -*- coding:utf-8 -*-\n",
    "# Author：wancong\n",
    "# Date: 2018-04-29\n",
    "from pyhanlp import *\n",
    "\n",
    "\n",
    "def demo_dependency_parser():\n",
    "    \"\"\" 依存句法分析（CRF句法模型需要-Xms512m -Xmx512m -Xmn256m，\n",
    "        MaxEnt和神经网络句法模型需要-Xms1g -Xmx1g -Xmn512m）\n",
    "        https://github.com/hankcs/pyhanlp\n",
    "    \"\"\"\n",
    "    sentence = HanLP.parseDependency(\"天喂饲食物的量要适度，过多过少都不利成长。\")\n",
    "#     print(dir())\n",
    "    for word in sentence.iterator():  # 通过dir()可以查看sentence的方法\n",
    "#         print(word.ID)\n",
    "#         print(word.HEAD.ID)\n",
    "        print((word.ID,word.LEMMA, word.DEPREL, word.HEAD.LEMMA,word.POSTAG,word.HEAD.ID))\n",
    "    print()\n",
    "\n",
    "    # 也可以直接拿到数组，任意顺序或逆序遍历\n",
    "    word_array = sentence.getWordArray()\n",
    "    \n",
    "    for word in word_array:\n",
    "#         print(\"%s --(%s)--> %s\" % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA))\n",
    "        print((word.LEMMA, word.DEPREL, word.HEAD.LEMMA,word.POSTAG,word.HEAD.ID))\n",
    "    print(len(word_array))\n",
    "\n",
    "    # 还可以直接遍历子树，从某棵子树的某个节点一路遍历到虚根\n",
    "    CoNLLWord = JClass(\"com.hankcs.hanlp.corpus.dependency.CoNll.CoNLLWord\")\n",
    "    head = word_array[0]\n",
    "    while head.HEAD:\n",
    "        head = head.HEAD\n",
    "        if (head == CoNLLWord.ROOT):\n",
    "            print(head.LEMMA)\n",
    "        else:\n",
    "            print(\"%s --(%s)--> \" % (head.LEMMA, head.DEPREL))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo_dependency_parser()\n",
    "#     import doctest\n",
    "#     doctest.testmod(verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordBean(object):\n",
    "    ''' 扩展conllword，存储父节点与孩子节点索引\n",
    "    https://zekizz.github.io/NLP/information-extraction/\n",
    "    '''\n",
    "    def __init__(self):\n",
    "       self.lemma = None\n",
    "       self.postag = None\n",
    "       self.relation = None\n",
    "       self.head_id = None\n",
    "       self.flag = True  # 是否还处于树中标志位，已合并的ATT将置为False\n",
    "       self.child = []\n",
    "\n",
    "    def set_word(self, conll_word):\n",
    "       self.lemma = conll_word.LEMMA\n",
    "       self.postag = conll_word.POSTAG\n",
    "       self.relation = conll_word.DEPREL\n",
    "       self.head_id = conll_word.HEAD.ID - 1\n",
    "\n",
    "    def add_child(self, child_id, child_relation):\n",
    "       self.child.append((child_id, child_relation))\n",
    "       \n",
    "   \n",
    "    def extract_entity_tuple(self, dependency_tree, seed_id, head_type=''):\n",
    "       '''自上而下解析树'''\n",
    "       res_entity_tuple_list = []\n",
    "       res_dict = dict()\n",
    "       res_dict['time'] = []\n",
    "       res_dict['reason'] = []\n",
    "       res_dict['place'] = []\n",
    "       res_dict['status'] = []\n",
    "       # 判断是否是叶子节点\n",
    "       if len(dependency_tree[seed_id].child) == 0:\n",
    "           if dependency_tree[seed_id].lemma in self.status_set:\n",
    "               res_dict['status'].append((seed_id, dependency_tree[seed_id].lemma))\n",
    "           elif self.check_is_time(dependency_tree, seed_id):\n",
    "               res_dict['time'].append((seed_id, dependency_tree[seed_id].lemma))\n",
    "           elif head_type == 'reason':\n",
    "               res_dict['reason'].append((seed_id, dependency_tree[seed_id].lemma))\n",
    "           elif head_type == 'time':\n",
    "               res_dict['time'].append((seed_id, dependency_tree[seed_id].lemma))\n",
    "           else:\n",
    "               if not dependency_tree[seed_id].lemma in self.discard_word_set:\n",
    "                   res_dict['place'].append((seed_id, dependency_tree[seed_id].lemma))\n",
    "           res_entity_tuple_list.append(res_dict)\n",
    "           return res_entity_tuple_list\n",
    "\n",
    "       # 非叶子节点需要向下递归解析\n",
    "       if dependency_tree[seed_id].lemma in self.status_set:\n",
    "           # 当前节点为状态节点\n",
    "           status_merge_list = []\n",
    "           for c_id, c_relation in dependency_tree[seed_id].child:\n",
    "               child_bean = dependency_tree[c_id]\n",
    "               if c_relation in ('COO', '并列关系'):\n",
    "                   # 假设状态下不存在嵌套状态，有视为补充\n",
    "                   if dependency_tree[c_id].postag == 'v' and len(dependency_tree[c_id].child) == 0:\n",
    "                       res_dict['status'].append((c_id, dependency_tree[c_id].lemma))\n",
    "                   else:\n",
    "                       child_dict_list = self.extract_entity_tuple(dependency_tree, c_id)\n",
    "                       for child_dict in child_dict_list:\n",
    "                           self.merge_two_tuple_dict(res_dict, child_dict)\n",
    "               elif c_relation in ('ADV', '状中结构'):\n",
    "                   # 处理状中结构\n",
    "                   if child_bean.lemma in ('因', '受', '由于'):\n",
    "                       # 处理原因\n",
    "                       child_dict_list = self.extract_entity_tuple(dependency_tree, c_id)\n",
    "                       for child_dict in child_dict_list:\n",
    "                           self.merge_two_tuple_dict(res_dict, child_dict)\n",
    "                   elif child_bean.lemma == '处' or child_bean.postag == 'p':\n",
    "                       child_dict_list = self.extract_entity_tuple(dependency_tree, c_id)\n",
    "                       for child_dict in child_dict_list:\n",
    "                           self.merge_two_tuple_dict(res_dict, child_dict)\n",
    "                   elif child_bean.postag in ('a', 'ad', 'd'):\n",
    "                       self.merge_att(dependency_tree, c_id)\n",
    "                       status_merge_list.append(c_id)\n",
    "                   elif self.check_is_time(dependency_tree, c_id):\n",
    "                       self.merge_att(dependency_tree, c_id)\n",
    "                       res_dict['time'].append((c_id, dependency_tree[c_id].lemma))\n",
    "               elif c_relation in ('POB', '介宾关系') and child_bean.lemma in ('因', '受', '由于'):\n",
    "                   self.merge_att(dependency_tree, c_id)\n",
    "                   res_dict['reason'].append((c_id, dependency_tree[c_id].lemma))\n",
    "               elif c_relation in ('CMP', '动补结构'):\n",
    "                   self.merge_att(dependency_tree, c_id)\n",
    "                   status_merge_list.append(c_id)\n",
    "               elif c_relation in ('SBV', '主谓关系'):\n",
    "                   # 处理主谓关系，解析具体地点\n",
    "                   child_dict_list = self.extract_entity_tuple(dependency_tree, c_id)\n",
    "                   for child_dict in child_dict_list:\n",
    "                       self.merge_two_tuple_dict(res_dict, child_dict)\n",
    "               elif c_relation in ('VOB', '动宾关系'):\n",
    "                   if len(dependency_tree[c_id].child) == 0:\n",
    "                       res_dict['status'].append((c_id, dependency_tree[c_id].lemma))\n",
    "                   else:\n",
    "                       child_dict_list = self.extract_entity_tuple(dependency_tree, c_id)\n",
    "                       for child_dict in child_dict_list:\n",
    "                           self.merge_two_tuple_dict(res_dict, child_dict)\n",
    "\n",
    "           status_buffer = []\n",
    "           status_merge_list.append(seed_id)\n",
    "           status_merge_list.sort()\n",
    "           for id in status_merge_list:\n",
    "               status_buffer.append(dependency_tree[id].lemma)\n",
    "           res_dict['status'].append((seed_id, ''.join(status_buffer)))\n",
    "\n",
    "           res_entity_tuple_list.append(res_dict)\n",
    "           return res_entity_tuple_list\n",
    "       else:\n",
    "           # 当前节点为非状态节点\n",
    "           pre_head_type = head_type\n",
    "           if self.check_is_time(dependency_tree, seed_id):\n",
    "               # 为时间节点\n",
    "               head_type = 'time'\n",
    "               res_dict['time'].append((seed_id, dependency_tree[seed_id].lemma))\n",
    "           elif dependency_tree[seed_id].lemma in ('因', '受', '由于') or head_type == 'reason':\n",
    "               # 为原因节点\n",
    "               head_type = 'reason'\n",
    "               res_dict['reason'].append((seed_id, dependency_tree[seed_id].lemma))\n",
    "           else:\n",
    "               if not dependency_tree[seed_id].lemma in self.discard_word_set:\n",
    "                   res_dict['place'].append((seed_id, dependency_tree[seed_id].lemma))\n",
    "           child_dict_list = []\n",
    "           coo_list = []\n",
    "           for c_id, c_relation in dependency_tree[seed_id].child:\n",
    "               # if c_relation in ('COO', '并列关系'):\n",
    "               #     coo_list.append(c_id)\n",
    "               if c_relation in ('WP', '标点符号'):\n",
    "                   continue\n",
    "               else:\n",
    "                   if head_type == 'reason' and not pre_head_type == 'reason':\n",
    "                       if c_relation in ('POB', '介宾关系'):\n",
    "                           child_dict_list.extend(self.extract_entity_tuple(dependency_tree, c_id, head_type))\n",
    "                           if dependency_tree[c_id].lemma in self.status_set:\n",
    "                               res_dict['reason'].append((c_id, dependency_tree[c_id].lemma))\n",
    "                   else:\n",
    "                       child_dict_list.extend(self.extract_entity_tuple(dependency_tree, c_id, head_type))\n",
    "\n",
    "           # 先合并非状态\n",
    "           status_dict_list = []\n",
    "           for child_dict in child_dict_list:\n",
    "               if len(child_dict['status']) > 0:\n",
    "                   status_dict_list.append(child_dict)\n",
    "               else:\n",
    "                   self.merge_two_tuple_dict(res_dict, child_dict)\n",
    "\n",
    "           # 再合并存在状态的\n",
    "           if len(status_dict_list) == 0:\n",
    "               res_entity_tuple_list.append(res_dict)\n",
    "           else:\n",
    "               for child_dict in status_dict_list:\n",
    "                   # tmp_dict = res_dict.copy()\n",
    "                   tmp_dict = copy.deepcopy(res_dict)\n",
    "                   self.merge_two_tuple_dict(tmp_dict, child_dict)\n",
    "                   res_entity_tuple_list.append(tmp_dict)\n",
    "\n",
    "           return res_entity_tuple_list\n",
    "\n",
    "\n",
    "    def extract_information(self, line):\n",
    "#        segs = self.nlp_tokenizer.seg(line)\n",
    "#        # fix segs\n",
    "#        self.fix_seged_postag(segs)\n",
    "#        conll_words = self.parser.parse(segs).getWordArray()\n",
    "       conll_words = line\n",
    "       dependency_tree, root_id = self.construct_dependency_tree(conll_words)\n",
    "\n",
    "       res_entity.append(self.extract_entity_tuple(dependency_tree, i))\n",
    "       res_entity = self.extract_entity_tuple(dependency_tree, root_id)\n",
    "\n",
    "       # print entity tuples\n",
    "       for entity in res_entity:\n",
    "           entity['time'].sort()\n",
    "           entity['place'].sort()\n",
    "           entity['reason'].sort()\n",
    "           entity['status'].sort()\n",
    "           self.fix_entity_tuple_dict(entity)\n",
    "           print(entity)\n",
    "       return res_entity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('天', '定中关系', '食物', 'n', 3)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'com.hankcs.hanlp.corpus.dependency.CoNll.CoNLLWord' object has no attribute 'child'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-3e075bdb2fb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# print(len(word_array))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#     print(w.extract_information(word_array))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_entity_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_entity_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-bc2f49a09455>\u001b[0m in \u001b[0;36mextract_entity_tuple\u001b[0;34m(self, dependency_tree, seed_id, head_type)\u001b[0m\n\u001b[1;32m     30\u001b[0m        \u001b[0mres_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'status'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m        \u001b[0;31m# 判断是否是叶子节点\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependency_tree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mdependency_tree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                \u001b[0mres_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'status'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdependency_tree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pan/github/ai_writer/lib/python3.6/site-packages/jpype/_jclass.py\u001b[0m in \u001b[0;36m_javaGetAttr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_jpype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_JavaMethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pan/github/ai_writer/lib/python3.6/site-packages/jpype/_jclass.py\u001b[0m in \u001b[0;36m_javaGetAttr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_javaGetAttr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__metaclass__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'com.hankcs.hanlp.corpus.dependency.CoNll.CoNLLWord' object has no attribute 'child'"
     ]
    }
   ],
   "source": [
    "sentence = HanLP.parseDependency(\"天喂饲食物的量要适度，过多过少都不利成长。\")\n",
    "w = WordBean()\n",
    "\n",
    "# print(w.extract_information(sentence))\n",
    "sentence = HanLP.parseDependency(\"天喂饲食物的量要适度，过多过少都不利成长。\")\n",
    "#     print(dir())\n",
    "# for word in sentence.iterator():  # 通过dir()可以查看sentence的方法\n",
    "# #         print(word.ID)\n",
    "# #         print(word.HEAD.ID)\n",
    "# #     print((word.ID,word.LEMMA, word.DEPREL, word.HEAD.LEMMA,word.POSTAG,word.HEAD.ID))\n",
    "#     print(w.extract_information(word))\n",
    "# # print()\n",
    "# word_array = sentence.getWordArray()\n",
    "\n",
    "# print(w.extract_information(word_array))\n",
    "\n",
    "word_array = sentence.getWordArray()\n",
    "\n",
    "for word in word_array:\n",
    "#         print(\"%s --(%s)--> %s\" % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA))\n",
    "    print((word.LEMMA, word.DEPREL, word.HEAD.LEMMA,word.POSTAG,word.HEAD.ID))\n",
    "# print(len(word_array))\n",
    "#     print(w.extract_information(word_array))\n",
    "    print(w.extract_entity_tuple(word_array, word.ID))\n",
    "print(w.extract_entity_tuple(word_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "E:\\ProgramData\\Anaconda3\\coreNLP\\stanford-corenlp-full-2016-10-31 is not a directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-1eace78c3d7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstanfordcorenlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStanfordCoreNLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'我叫小米'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mStanfordCoreNLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'E:\\ProgramData\\Anaconda3\\coreNLP\\stanford-corenlp-full-2016-10-31'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zh'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# ---------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pan/github/ai_writer/lib/python3.6/site-packages/stanfordcorenlp/corenlp.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_host, port, memory, lang, timeout, quiet, logging_level)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# Check if the dir exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_or_host\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_or_host\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' is not a directory.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_or_host\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_path_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: E:\\ProgramData\\Anaconda3\\coreNLP\\stanford-corenlp-full-2016-10-31 is not a directory."
     ]
    }
   ],
   "source": [
    "# from nltk.tree import Tree\n",
    "# from stanfordcorenlp import StanfordCoreNLP\n",
    "# sentence = '我叫小米'\n",
    "# with StanfordCoreNLP(r'E:\\ProgramData\\Anaconda3\\coreNLP\\stanford-corenlp-full-2016-10-31', lang='zh') as nlp:\n",
    "#     Tree.fromstring(nlp.parse(sentence)).draw()\n",
    "# # --------------------- \n",
    "# # 作者：闰土不用叉 \n",
    "# # 来源：CSDN \n",
    "# # 原文：https://blog.csdn.net/xyz1584172808/article/details/81951846 \n",
    "# # 版权声明：本文为博主原创文章，转载请附上博文链接！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from harvesttext import HarvestText\n",
    "ht = HarvestText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.692 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "ename": "java.util.NoSuchElementExceptionPyRaisable",
     "evalue": "java.util.NoSuchElementException",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mjava.util.NoSuchElementExceptionPyRaisable\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-aa6b2b6cf5bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mentity_type_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'武磊'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'球员'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"上海上港\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"球队\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mht0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_mention_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity_type_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0marc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mht0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdependency_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mht0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriple_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pan/github/ai_writer/lib/python3.6/site-packages/harvesttext/harvesttext.py\u001b[0m in \u001b[0;36mdependency_parse\u001b[0;34m(self, sent, standard_name, stopwords)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHanLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseDependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mword0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEMMA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOSTAG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mjava.util.NoSuchElementExceptionPyRaisable\u001b[0m: java.util.NoSuchElementException"
     ]
    }
   ],
   "source": [
    "ht0 = HarvestText()\n",
    "para = \"上港的武磊武球王是中国最好的前锋。\"\n",
    "entity_mention_dict = {'武磊': ['武磊', '武球王'], \"上海上港\":[\"上港\"]}\n",
    "entity_type_dict = {'武磊': '球员', \"上海上港\":\"球队\"}\n",
    "ht0.add_entities(entity_mention_dict, entity_type_dict)\n",
    "for arc in ht0.dependency_parse(para):\n",
    "    print(arc)\n",
    "print(ht0.triple_extraction(para))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
