原标题：干货！ 
：专注为AI开发者提供全球最新AI技术动态和社群交流。
用户来源包括：北大、清华、中科院、复旦、麻省理工、卡内基梅隆、斯坦福、哈佛、牛津、剑桥等世界名校的AI技术硕士、博士和教授；以及谷歌、腾讯、百度、脸谱、微软、华为、阿里、海康威视、滴滴、英伟达等全球名企的AI开发者和AI科学家。 
Incrementally**”。
它主要解决了一个深度学习中的重要问题：如何使用尽可能少的标签数据来训练一个效果promising的分类器。 
B一般发生在高富帅公司，有海量的精标记数据，但是由于目前即使是最牛逼的计算机也不能用深度学习在短时间内一次性地去处理完这些数据（e.g.，内存溢出，或者算上个几年都算不完）。
Anyway，我想我已经说清楚应用背景了，读者可以根据实际情况判断是否往后读下去。 
感谢你选择继续往下阅读，那么如果觉得下文所传递的思想和方法对你有帮助，请记得一定引用这篇CVPR2017的文章。 
**1. 为什么会想到去解决这个问题？ 
现在深度学习很火，做的人也越来越多，那么它的门槛可以说是很低的，Caffe，Keras，Torch等等框架的出现，让该领域的programming的门槛直接拆了。
所以深度学习真正的门槛变成了很简单概念——钱。 
Power），二是标记数据的数量。
这儿就引出一个很重要的问题：是不是训练数据集越多，深度学习的效果会越好呢？
这个答案凭空想是想不出来的，能回答的人一定是已经拥有了海量的数据，如ImageNet，Place等等，他们可以做一系列的实验来回答这个问题。 
truth就是在浪费时间和金钱。
有了这个认知，接下来就是想办法让这个临界值变小，也就是用更小的训练集来更快地达到最理想的性能，如右图的红虚线所示。 
Learning）的手段来增加训练集，从而找到一个更小的子集来达到最理想的性能。 
这里需要说明的一点是，训练样本数的临界点大小和这个分类问题的难度有关，如果这个分类问题非常简单，如黑白图像分类（白色的是1，黑色的是0），那么这个临界值就特别小，往往几幅图就可以训练一个精度很高的分类器；如果分类问题很复杂，如判断一个肿瘤的良恶性（良性是0，恶性是1），那么这个临界值会很大，因为肿瘤的形状，大小，位置各异，分类器需要学习很多很多的样本，才能达到一个比较稳定的性能。 
对于很多从事深度学习的无论是研究员还是企业家都是一个十分有启发性的认知改变。
一般来讲，人的惯性思维会引领一个默认的思路，就是训练样本越多越好，如左图所示，这将直接导致许多工作的停滞不前，理由永远是“我们没有足够的数据，怎么训练网络！
”进一步的思路是图二的红实线认知：要多少是多啊，先训着再说，慢慢的就会发现即便用自己有的一小部分数据集好像也能达到一个不错的分类性能，这个时候就遇到一个问题：自己的数据集样本数到底有没有到达临界值呢？
这个问题也很关键，它决定了要不要继续花钱去找人标注数据了。
这个问题我会在第三部分去回答它，这里先假设我们知道了它的答案，接下来的问题就是如何让这个临界值变小？ 
**2\. 如何让临界值变小？ 
mining）。 
sample对于提升分类器效果最有效而快速。 
sample？
或者说怎么去描述当前分类器对于不同样本的分类结果的好坏？ 
由于深度学习的输出是属于某一类的概率（0～1），一个很直观的方法就是用“熵（entropy）”来刻画信息量，把那些预测值模棱两可的样本挑出来，对于二分类问题，就是预测值越靠近0.5，它们的信息量越大。 
augmentation）的时候，往往是知道哪些标签应该是属于一类的，尽管不知道具体是哪一类。
举例来讲：对于一幅图我们进行平移变换，我们知道它们其实是来自同一幅图的，那么最后的预测应该是一致的，如果当前网络对于这写patch的预测值偏差很大，那么我们认为这个样本是比较难分类的，需要被挑出来作为训练集。 
bounds“中被提出。
是十分重要的两个Active Learning的选择指标。 
有了这两个指标来选hard sample，是比较靠谱了——实验结果表明，这比随机去选已经能更快地达到临界拐点了。 
**FIG.2 Active Learning的结构示意图。
利用深度学习所带来的优势在于，一开始你可以不需要有标记的数据集。 
假设你是一个养狗的大户，你现在想做一个非常偏的（专业化的）分类问题，如卷毛比雄犬和哈瓦那犬的分类问题，你手头有这两种狗各50条，你自己可以很轻松地区分这100条狗，现在需要做的是训练一个分类器，给那些不懂狗的人，他们拍了狗的照片然后把照片输入到这个分类器就可以很好地判断这是卷毛比雄犬还是哈瓦那犬。
首先你可以给这100条狗拍照片，每条狗都有不同形态的10张照片，一共拍了1000张没有标记的照片。
对于这1000张照片，你所知道的是哪10张对应的是一条狗，其他什么都不知道。 
Image也不能企及，当然一种选择是你把1000张图片从头到尾看一遍，标注好，但是你更希望是把大多数简单的分类工作交给分类器，自己尽可能少的做标记工作，并且主要是去标记那些分类器模棱两可的那几张照片来提高分类器的性能。 
如 **FIG.2** 所示，每次循环都用不断增加的标记数据集去提升分类器的性能，每次都挑对当前分类器比较难的样本来人为标记。 
**3\. 这个过程什么时候可以停？ 
以上三种情况都可以让这个循环训练过程中断，第一种就很无奈了，没钱找人标记了...第二种情况和第三种情况的前提共识是如果难的样本都分类正确了，那么我们认为简单的样本肯定也基本上分类正确了，即便不知道标签。
第三种情况，举例来说就是黑白图像分类，结果分类器模棱两可的图像是灰的...也就是说事实上的确分不了，并且当前的分类器居然能把分不了的样本也找出来，这时我们认为这个分类器的性能已经不错的了，所以循环训练结束。



